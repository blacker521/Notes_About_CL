---
typora-root-url: img
---

# [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

Word2Vec原文

提出了两种新的模型结构，用于计算非常大数据集中单词的连续矢量表示。这些表示的质量是在一个词相似性任务中测量的，并将结果与以前基于不同类型神经网络的最佳表现技术进行比较。

1. Word2Vec两个算法模型的原理是什么，网络结构怎么画？
2. 网络的输入输出是什么？隐藏层的激活函数是什么？输出层的激活函数是什么？
3. Loss？
4. Word2Vec如何获取词向量？
5. 参数如何更新？
6. 分层的softmax怎么做的？二叉树，左右两个概率
7. Word2Vec的参数有哪些？
8. 局限性？
## 编码方式
### One-Hot

以字符变量的种类作为向量长度，向量中仅有一个元素为1，其余为0，数据稀疏，不适合作为网络的输入，且不能显示词与词之间的关系

### 分布式编码

把字符变量映射到固定长度的向量中，向量空间中的表示字符，且字符间的距离是有意义的，越相似，越相近

![](/n_1001.png)

COBW用上下文预测当前单词，Skip-Gram用当前次预测上下文

![](/n_1002.png)

**网络的输入是One-Hot向量wk，隐藏层没有激活函数，输出层有Softmax函数，输出的是概率分布，预测目标为One-Hot向量wj。层与层之间是全连接的**

![](/n_1003.png)

![](/n_1004.png)

## CBOW

![](/n_1005.png)
## Skip-Gram
![](/n_1006.png)
![](/n_1007.png)
## 分层的Softmax
使用哈夫曼二叉树的**叶节点**来表示语料库的所有单词
![](/n_1008.png)
![](/n_1009.png)
![](/n_1010.png)
![](/n_1011.png)
## 负采样
以一定的概率选取负样本，使得每次迭代只需要修改一部分参数，给定一些变量及其概率，随机采样使得其满足变量出现的概率。
节省了计算量
保证了模型训练的效果，其一模型每次只需要更新采样的词的权重，不用更新所有的权重，那样会很慢，其二中心词其实只跟它周围的词有关系，位置离着很远的词没有关系，也没必要同时训练更新
negative sampling 每次让一个训练样本仅仅更新一小部分的权重参数，从而降低梯度下降过程中的计算量。
如果 vocabulary 大小为1万时， 当输入样本 ( "fox", "quick") 到神经网络时， “ fox” 经过 one-hot 编码，在输出层我们期望对应 “quick” 单词的那个神经元结点输出 1，其余 9999 个都应该输出 0。在这里，这9999个我们期望输出为0的神经元结点所对应的单词我们为 negative word. negative sampling 的想法也很直接 ，将随机选择一小部分的 negative words，比如选 10个 negative words 来更新对应的权重参数。
![](/n_1012.png)